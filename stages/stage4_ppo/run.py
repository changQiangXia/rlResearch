"""
PPO Training Script (Stage 4)
ä½¿ç”¨å¹¶è¡Œç¯å¢ƒå……åˆ†åˆ©ç”¨ GPU
"""
import os
import sys
import numpy as np
import gym
import torch

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from stages.stage4_ppo.agent import PPOAgent
from common.logger import Logger
from utils.visualization import plot_training_curves, plot_metrics


def train(
    env_name: str = 'Acrobot-v1',
    num_envs: int = 16,          # å¹¶è¡Œç¯å¢ƒæ•°
    num_steps: int = 256,        # æ¯è½®æ”¶é›†æ­¥æ•°
    total_updates: int = 500,    # æ€»æ›´æ–°æ¬¡æ•°
    num_epochs: int = 10,        # æ¯æ¬¡æ•°æ®è®­ç»ƒè½®æ•°
    batch_size: int = 256,       # Mini-batch å¤§å°
    
    hidden_dims: list = [512, 512],
    learning_rate: float = 3e-4,
    gamma: float = 0.99,
    gae_lambda: float = 0.95,
    clip_epsilon: float = 0.2,
    value_coef: float = 0.5,
    entropy_coef: float = 0.01,
    max_grad_norm: float = 0.5,
    
    log_interval: int = 10,
    save_interval: int = 100,
    results_dir: str = None,
):
    """
    PPO è®­ç»ƒä¸»å‡½æ•°
    
    Args:
        num_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆæ¨è 8-16ï¼‰
        num_steps: æ¯æ¬¡æ”¶é›†çš„æ­¥æ•°ï¼ˆæ¨è 128-512ï¼‰
        total_updates: æ€»æ›´æ–°æ¬¡æ•°
        num_epochs: æ¯ä¸ªæ•°æ®é›†çš„è®­ç»ƒè½®æ•°ï¼ˆPPO ç‰¹æœ‰ï¼Œæ¨è 3-10ï¼‰
        batch_size: Mini-batch å¤§å°
    """
    if results_dir is None:
        results_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'results', 'stage4')
    os.makedirs(results_dir, exist_ok=True)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ PPO Training - Stage 4")
    print(f"Device: {device}")
    print(f"Parallel envs: {num_envs}")
    print(f"Steps per update: {num_steps * num_envs}")
    print(f"PPO epochs: {num_epochs}")
    print("-" * 70)
    
    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    envs = [gym.make(env_name) for _ in range(num_envs)]
    state_dim = envs[0].observation_space.shape[0]
    action_dim = envs[0].action_space.n
    
    print(f"Environment: {env_name}")
    print(f"State dim: {state_dim}, Action dim: {action_dim}")
    print("-" * 70)
    
    # åˆ›å»º PPO Agent
    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dims=hidden_dims,
        learning_rate=learning_rate,
        gamma=gamma,
        gae_lambda=gae_lambda,
        clip_epsilon=clip_epsilon,
        value_coef=value_coef,
        entropy_coef=entropy_coef,
        max_grad_norm=max_grad_norm,
        device=device,
    )
    
    # åˆ›å»º Logger
    logger = Logger(
        log_dir=os.path.join(results_dir, 'logs'),
        use_tensorboard=True,
    )
    
    # åˆå§‹åŒ–
    states = np.array([env.reset() for env in envs])
    episode_rewards = [0.0] * num_envs
    episode_counts = [0] * num_envs
    all_episode_rewards = []
    
    print(f"{'Update':>8} | {'Episodes':>10} | {'AvgReward':>10} | {'Policy':>10} | {'Value':>10} | {'ClipFrac':>8} | {'ExpVar':>8}")
    print("-" * 85)
    
    for update in range(1, total_updates + 1):
        # å­˜å‚¨æ•°æ®
        batch_states = np.zeros((num_steps, num_envs, state_dim), dtype=np.float32)
        batch_actions = np.zeros((num_steps, num_envs), dtype=np.int64)
        batch_log_probs = np.zeros((num_steps, num_envs), dtype=np.float32)
        batch_rewards = np.zeros((num_steps, num_envs), dtype=np.float32)
        batch_values = np.zeros((num_steps, num_envs), dtype=np.float32)
        batch_dones = np.zeros((num_steps, num_envs), dtype=np.float32)
        
        # æ”¶é›†æ•°æ®
        for step in range(num_steps):
            batch_states[step] = states
            
            # å¹¶è¡Œé€‰æ‹©åŠ¨ä½œ
            actions, log_probs, _, values = zip(*[agent.select_action(s) for s in states])
            batch_actions[step] = actions
            batch_log_probs[step] = log_probs
            batch_values[step] = values
            
            # æ‰§è¡ŒåŠ¨ä½œ
            for i, env in enumerate(envs):
                next_state, reward, done, _ = env.step(actions[i])
                batch_rewards[step, i] = reward
                batch_dones[step, i] = float(done)
                
                episode_rewards[i] += reward
                
                if done:
                    all_episode_rewards.append(episode_rewards[i])
                    episode_rewards[i] = 0.0
                    episode_counts[i] += 1
                    states[i] = env.reset()
                else:
                    states[i] = next_state
        
        # è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        with torch.no_grad():
            next_states_tensor = torch.FloatTensor(states).to(device)
            _, next_values = agent.network(next_states_tensor)
            next_values = next_values.squeeze().cpu().numpy()
        
        # è®¡ç®— GAE
        advantages = np.zeros((num_steps, num_envs), dtype=np.float32)
        returns = np.zeros((num_steps, num_envs), dtype=np.float32)
        
        for env_id in range(num_envs):
            adv, ret = agent.compute_gae(
                batch_rewards[:, env_id],
                batch_values[:, env_id],
                batch_dones[:, env_id],
                next_values[env_id]
            )
            advantages[:, env_id] = adv
            returns[:, env_id] = ret
        
        # å±•å¹³æ•°æ® (num_steps, num_envs) -> (num_steps * num_envs,)
        batch_states = batch_states.reshape(-1, state_dim)
        batch_actions = batch_actions.reshape(-1)
        batch_log_probs = batch_log_probs.reshape(-1)
        advantages = advantages.reshape(-1)
        returns = returns.reshape(-1)
        
        # PPO æ›´æ–°
        loss_dict = agent.update(
            batch_states, batch_actions, batch_log_probs,
            advantages, returns,
            num_epochs=num_epochs,
            batch_size=batch_size
        )
        
        # æ—¥å¿—
        if update % log_interval == 0:
            avg_reward = np.mean(all_episode_rewards[-100:]) if len(all_episode_rewards) >= 100 else np.mean(all_episode_rewards) if all_episode_rewards else 0.0
            total_episodes = sum(episode_counts)
            
            print(f"{update:8d} | {total_episodes:10d} | {avg_reward:10.2f} | "
                  f"{loss_dict['policy_loss']:10.4f} | {loss_dict['value_loss']:10.4f} | "
                  f"{loss_dict['clip_fraction']:8.4f} | {loss_dict['explained_variance']:8.4f}")
            
            # è®°å½•åˆ° TensorBoard
            logger.log_scalar('train/avg_reward', avg_reward, update)
            logger.log_scalar('train/policy_loss', loss_dict['policy_loss'], update)
            logger.log_scalar('train/value_loss', loss_dict['value_loss'], update)
            logger.log_scalar('train/entropy', loss_dict['entropy'], update)
            logger.log_scalar('train/clip_fraction', loss_dict['clip_fraction'], update)
            logger.log_scalar('train/explained_variance', loss_dict['explained_variance'], update)
        
        # ä¿å­˜æ¨¡å‹
        if update % save_interval == 0:
            model_path = os.path.join(results_dir, f'ppo_model_update{update}.pth')
            agent.save(model_path)
    
    print("-" * 85)
    print("Training completed!")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(results_dir, 'ppo_model_final.pth')
    agent.save(final_model_path)
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    plot_training_curves(
        all_episode_rewards,
        save_path=os.path.join(results_dir, 'training_rewards.png'),
        title='PPO Training Rewards'
    )
    
    # ä¿å­˜ PPO ç‰¹æœ‰æŒ‡æ ‡
    metrics_dict = {
        'policy_loss': agent.policy_losses,
        'value_loss': agent.value_losses,
        'entropy': agent.entropy_losses,
        'clip_fraction': agent.clip_fractions,
        'explained_variance': agent.explained_variances,
    }
    plot_metrics(metrics_dict, save_path=os.path.join(results_dir, 'training_metrics.png'))
    
    # å…³é—­ç¯å¢ƒ
    for env in envs:
        env.close()
    logger.close()
    
    return agent, all_episode_rewards


if __name__ == '__main__':
    # PPO è®­ç»ƒé…ç½®
    agent, rewards = train(
        env_name='Acrobot-v1',
        num_envs=16,        # 16 å¹¶è¡Œç¯å¢ƒ
        num_steps=256,      # æ¯è½® 256 æ­¥
        total_updates=500,  # å…± 500 æ¬¡æ›´æ–°
        num_epochs=10,      # æ¯æ‰¹æ•°æ®è®­ç»ƒ 10 è½®ï¼ˆPPO æ ¸å¿ƒï¼‰
        batch_size=256,     # Mini-batch å¤§å°
        
        hidden_dims=[512, 512],
        learning_rate=3e-4,
        clip_epsilon=0.2,   # PPO clipping
        entropy_coef=0.01,
    )
    
    print(f"\nFinal Stats:")
    print(f"  Total episodes: {len(rewards)}")
    print(f"  Avg last 100: {np.mean(rewards[-100:]):.2f}")
    print(f"  Best: {max(rewards):.2f}")
